# model_utils.py

import os
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np

# ìì› ì‚¬ìš©ëŸ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
torch.set_num_threads(2)  # Intel Core i5-6200UëŠ” 2ì½”ì–´/4ìŠ¤ë ˆë“œ

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ë° ê´€ë¦¬ í•¨ìˆ˜
def check_memory_usage():
    import psutil
    process = psutil.Process(os.getpid())
    mem = process.memory_info().rss / 1024 / 1024
    return mem

# ğŸ§© ì»¤ìŠ¤í…€ Dataset í´ë˜ìŠ¤
class MessageDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        premise = f"'{row['relationship']}' ê´€ê³„ì—ì„œ ë©”ì‹œì§€ê°€ ì í•©í•œê°€ìš”?"
        hypothesis = row['message']

        enc = self.tokenizer(
            premise,
            hypothesis,
            max_length=self.max_length,
            truncation=True,
            padding='max_length',
            return_tensors="pt"
        )
        return {
            'input_ids': enc['input_ids'].squeeze(0),
            'attention_mask': enc['attention_mask'].squeeze(0),
            'labels': torch.tensor(row['label'], dtype=torch.long)
        }

# ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬
def load_data(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    df = pd.read_csv(file_path, encoding="utf-8")
    df = df.rename(columns={
        "ë¬¸ì¥": "message",
        "ê´€ê³„": "relationship",
        "ì ì ˆë„": "label"
    })

    # 1) ë¬¸ìì—´ ê³µë°± ì œê±°
    df["label"] = df["label"].astype(str).str.strip()
    # 2) ìˆ«ìë¡œ ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ê°’ì€ NaN ì²˜ë¦¬
    df["label"] = pd.to_numeric(df["label"], errors="coerce")
    # 3) NaN(í—¤ë”ë‚˜ ê¹¨ì§„ ê°’) í–‰ë“¤ í†µì§¸ë¡œ ì œê±°
    df = df.dropna(subset=["label"])
    # 4) ì•ˆì „í•˜ê²Œ ì •ìˆ˜í˜•ìœ¼ë¡œ ìºìŠ¤íŒ…
    df["label"] = df["label"].astype(int)

    required = ["message", "relationship", "label"]
    if not all(col in df.columns for col in required):
        missing = [c for c in required if c not in df.columns]
        raise ValueError(f"CSVì— ëˆ„ë½ëœ ì»¬ëŸ¼: {missing}")

    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")
    return df

# ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc}

# ê°„ë‹¨ ì˜¤ë²„ìƒ˜í”Œë§
def simple_oversample(X, y):
    tmp = pd.concat([X, pd.Series(y, name='label')], axis=1)
    counts = tmp['label'].value_counts()
    max_n = counts.max()
    frames = []
    for val, cnt in counts.items():
        dfc = tmp[tmp['label'] == val]
        mul = max_n // cnt
        rem = max_n % cnt
        oversampled = pd.concat([dfc]*mul + [dfc.sample(rem, replace=True)])
        frames.append(oversampled)
    all_df = pd.concat(frames).sample(frac=1).reset_index(drop=True)
    return all_df[X.columns], all_df['label']

# ğŸ‹ï¸ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
def train_model(
    train_file,
    model_name: str = 'snunlp/KR-FinBERT',
    batch_size: int = 2,
    epochs: int = 2,
    output_dir: str = "./model_output"
):
    df = load_data(train_file)
    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)
    X_train, y_train = simple_oversample(
        train_df[['message','relationship']],
        train_df['label']
    )
    train_df = pd.concat([X_train, pd.Series(y_train, name='label')], axis=1)

    print(f"ğŸ“Š ì „ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬: {check_memory_usage():.2f} MB")

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, num_labels=3, trust_remote_code=True
    )
    model.gradient_checkpointing_enable()

    train_ds = MessageDataset(train_df, tokenizer)
    val_ds   = MessageDataset(val_df,   tokenizer)

    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=epochs,
        learning_rate=2e-5,

        logging_dir='./logs',
        logging_steps=10,
        save_total_limit=1,
        dataloader_num_workers=1,
        fp16=False,
        optim='adamw_torch'
    )
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    print("ğŸš€ í•™ìŠµ ì‹œì‘...")
    trainer.train()
    print("âœ… í•™ìŠµ ì™„ë£Œ!")

    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"ğŸ“¦ ì €ì¥ ì™„ë£Œ: {output_dir}")

# ğŸ§ª ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
def evaluate_model(test_file, model_dir):
    df = load_data(test_file)
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)
    tok   = AutoTokenizer.from_pretrained(model_dir)
    correct = 0
    for i in range(0, len(df), 4):
        batch = df.iloc[i:i+4]
        for _, row in batch.iterrows():
            prem = f"'{row['relationship']}' ê´€ê³„ì—ì„œ ë©”ì‹œì§€ê°€ ì í•©í•œê°€ìš”?"
            hyp  = row['message']
            enc  = tok(prem, hyp, return_tensors="pt", max_length=128,
                       truncation=True, padding='max_length')
            with torch.no_grad():
                out = model(**enc)
            pred = torch.argmax(out.logits, dim=-1).item()
            if pred == row['label']:
                correct += 1
    acc = correct/len(df)*100
    print(f"ğŸ“Š í‰ê°€ ì •í™•ë„: {acc:.2f}%")
    return acc

# ì „ì—­ ë³€ìˆ˜ ì„ ì–¸ (inference)
model = None
tokenizer = None

# ëª¨ë¸ ë¡œë“œ (inference)
def load_model(model_dir):
    global model, tokenizer
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    return True

# ì˜ˆì¸¡ í•¨ìˆ˜ (inference)
def predict(text: str, relationship: str = "ì¼ë°˜"):
    if model is None or tokenizer is None:
        raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    prem = f"'{relationship}' ê´€ê³„ì—ì„œ ë©”ì‹œì§€ê°€ ì í•©í•œê°€ìš”?"
    enc  = tokenizer(prem, text,
                     truncation=True, padding=True,
                     return_tensors="pt", max_length=128)
    with torch.no_grad():
        out = model(**enc)
    logits     = out.logits
    pred_idx   = torch.argmax(logits, dim=-1).item()
    probs      = torch.nn.functional.softmax(logits, dim=-1)
    confidence = probs[0][pred_idx].item() * 100
    return pred_idx, confidence

# ê²°ê³¼ í•´ì„ (inference)
def interpret_result(pred_idx: int) -> str:
    return {0: "ë¶€ì ì ˆ í‘œí˜„", 1: "ì¤‘ë¦½ í‘œí˜„", 2: "ì ì ˆ í‘œí˜„"}.get(pred_idx, "ì•Œ ìˆ˜ ì—†ìŒ")
